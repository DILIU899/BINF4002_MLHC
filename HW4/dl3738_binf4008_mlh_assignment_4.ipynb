{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3ECS6xY6S6I"
      },
      "source": [
        "# BINF GU 4002: Machine Learning for Healthcare, Spring 2025\n",
        "# Assignment \\#4: Conceptual Foundations and Limitations of Foundation Models\n",
        "## DUE: 11:59 PM, Tuesday, April 29, 2025\n",
        "\n",
        "This assignment is an hands-on exploration of frontier large-scale pretrained models, also known as \"foundation models\". The goal is to build intuition around these models by connecting concepts and ideas previously explored in class. As an illustrative example, you will be using tabular electronic health record data from the previous homework to finetune a state-of-the-art LLM for mortality prediction. The assignment is designed to be more open-ended and a chance to explore some of the literature in this field.\n",
        "\n",
        "**<font color=\"red\">Instructions: Please run the notebook using Google Colab to prevent any dependency / package issues and use the GPU runtime type provided by Colab. Make sure that your written answers are formatted using </font>$\\LaTeX$<font color=\"red\"> in `markdown` cells. When submitting, please name your files `{UNI}_binf4008_mlh_assignment_4.{filetype}` and submit a `.ipynb` version of your Jupyter notebook. </font>**\n",
        "\n",
        "Important: For this assignment you will have to change the runtime type to use GPUs in Colab (the Unsloth package requires GPUs for speedup). In the top-right menu, change the runtime type to T4 GPU before running any of the code below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCrcQI9beHHO"
      },
      "source": [
        "## [30 Points] Question 1: Preliminaries\n",
        "\n",
        "We will be using a pretrained LLM by Meta AI [Llama](https://arxiv.org/pdf/2302.13971), which is based on the GPT architecture.\n",
        "\n",
        "From the paper (https://arxiv.org/pdf/2302.13971):\n",
        "\n",
        "> We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions\n",
        "of tokens, and show that it is possible to train\n",
        "state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets.\n",
        "\n",
        "The huggingface repository contains documentation and pretrained model weights, which can be used for finetuing or for inference out-of-the box: https://huggingface.co/docs/transformers/main/en/model_doc/llama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### [15 Points] 1.1: From the Llama paper (linked above), list and explain some of their datasets used for pre-training. What type of text data is included in these datasets (ex. CommonCrawl)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font color=\"red\">Answer 1.1</font>\n",
        "\n",
        "- English CommonCrawl [67%]\n",
        "- C4 [15%]\n",
        "  - diverse pre-processed CommonCrawldatasets\n",
        "- Github [4.5%]\n",
        "  - code data, projects that are distributed under the Apache, BSD and MIT licenses\n",
        "- Wikipedia [4.5%]\n",
        "  - covering 20 languages, which use either the Latin or Cyrillic scripts: bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk.\n",
        "- Gutenberg and Books3 [4.5%]\n",
        "  - book corpora, the Gutenberg Project contains books that are in the public domain, and the Books3 section of ThePile\n",
        "- ArXiv [2.5%]  \n",
        "  - arXiv Latex files to add scientific data to the dataset\n",
        "- Stack Exchange [2%]   \n",
        "  - high quality questions and answers that covers a diverse set of domains, ranging from computer science to chemistry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### [15 Points] 1.2 Based on the datasets used, name one target domain and corresponding inference task you think the model will fail to generalize to? Think in terms of the pretraining data distribution over possible tokens $p_{train}(X)$ versus your example domain $p_{target}(X)$ and the density ratio $\\frac{p_{target}(X)}{p_{train}(X)}$ for characterizing distribution overlap. (We encourage you to use examples from your own research)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font color=\"red\">Answer 1.2</font>\n",
        "\n",
        "Drug-Protein interaction prediction. First, this field is not a real NLP task, so unless the model can learn the real world physics (which it can't if only learning from natural language), the model cannot really understand how to predict the interaction. Then, thinking about the pre-training itself, all the data are publicly available, which is not true to this field. High quality and high throughput drug-protein interaction data are from large pharmaceutical companies, even not from the academia. To get more scientific sense, the arxiv copora is not diverse enough for the LLM as not all papers will be submitted to arxiv, and some fields have their common pactice to submit the preprint to other websites, like bioarxiv and medicalarxiv, as a result of which, the scientific sense learned by LLM may also biased to some particular fields, like CS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gmQBQJeFpE9"
      },
      "source": [
        "#### Text Serialization and Tabular Classification\n",
        "\n",
        "We will revisit the MIMIC dataset that was used in HW3. The first objective will be to finetune the Llama model to perform toy prediction tasks in the form of Q and A.\n",
        "\n",
        "LLMs have shown to solve many tabular classificaiton and regression problems at scale, due to its capabilities to encode information across tasks. We will use a tokenization strategy known as \"text serialization\" (1, 2), which converts tabular data into a language format and tokenized using pre-existing vanilla tokenizers. Developing tokenization strategies for various data types (continuous measurements, time, multi-modal data) is an active research field that our department also works on.\n",
        "\n",
        "- [1] TabLLM: Few-shot Classification of Tabular Data with Large Language\n",
        "Models: https://arxiv.org/pdf/2210.10723\n",
        "- [2] Large Scale Transfer Learning for Tabular Data\n",
        "via Language Modeling: https://arxiv.org/pdf/2406.12031\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cDBZuJ9Ct7U",
        "outputId": "fc7c63ea-5a6d-4dcc-df60-7dbcd623fd29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from transformers import DataCollatorWithPadding, TrainingArguments\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we do the down sampling to make the computation managable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1000, 9)\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"./data/processed_mimic_sample.csv\", index_col=0)\n",
        "data = df.copy()\n",
        "data.drop_duplicates(\"subject_id\", inplace=True)\n",
        "data.set_index(\"subject_id\", inplace=True, drop=True)\n",
        "# we want 500 positive and 500 negative samples\n",
        "negative_index = data[data[\"label\"] == 0].sample(500, random_state=42).index.to_list()\n",
        "positive_index = data[data[\"label\"] == 1].sample(500, random_state=42).index.to_list()\n",
        "data = data.loc[positive_index + negative_index].copy()\n",
        "\n",
        "data[data.select_dtypes(include='number').columns] = data.select_dtypes(include='number').round(0).astype('Int64') # Round measurements\n",
        "\n",
        "print(data.shape)\n",
        "# If your dataset has too many rows (say over 10000 patients) and columns (say over 50 features) it will take more compute for inference.\n",
        "# I have found around 1000 patients (with balanced classes) and 10-20 features can work well as a proof-of-concept with Colab's GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>log_stay_day</th>\n",
              "      <th>admit_year</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>admission_type</th>\n",
              "      <th>insurance</th>\n",
              "      <th>language</th>\n",
              "      <th>race</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>subject_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16015722</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2138</td>\n",
              "      <td>F</td>\n",
              "      <td>86</td>\n",
              "      <td>EW EMER.</td>\n",
              "      <td>Medicare</td>\n",
              "      <td>English</td>\n",
              "      <td>BLACK/AFRICAN AMERICAN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11586654</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2125</td>\n",
              "      <td>M</td>\n",
              "      <td>70</td>\n",
              "      <td>DIRECT EMER.</td>\n",
              "      <td>Medicare</td>\n",
              "      <td>English</td>\n",
              "      <td>WHITE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14039848</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2153</td>\n",
              "      <td>F</td>\n",
              "      <td>56</td>\n",
              "      <td>EW EMER.</td>\n",
              "      <td>Medicaid</td>\n",
              "      <td>English</td>\n",
              "      <td>WHITE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15103745</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2185</td>\n",
              "      <td>F</td>\n",
              "      <td>81</td>\n",
              "      <td>EW EMER.</td>\n",
              "      <td>Medicare</td>\n",
              "      <td>English</td>\n",
              "      <td>BLACK/AFRICAN AMERICAN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16077953</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2182</td>\n",
              "      <td>M</td>\n",
              "      <td>91</td>\n",
              "      <td>EW EMER.</td>\n",
              "      <td>Medicare</td>\n",
              "      <td>English</td>\n",
              "      <td>WHITE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14551157</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2161</td>\n",
              "      <td>F</td>\n",
              "      <td>52</td>\n",
              "      <td>EU OBSERVATION</td>\n",
              "      <td>Private</td>\n",
              "      <td>English</td>\n",
              "      <td>WHITE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13273553</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2134</td>\n",
              "      <td>F</td>\n",
              "      <td>65</td>\n",
              "      <td>EW EMER.</td>\n",
              "      <td>Medicare</td>\n",
              "      <td>English</td>\n",
              "      <td>WHITE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15195372</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2117</td>\n",
              "      <td>F</td>\n",
              "      <td>21</td>\n",
              "      <td>OBSERVATION ADMIT</td>\n",
              "      <td>Private</td>\n",
              "      <td>English</td>\n",
              "      <td>OTHER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17238479</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2148</td>\n",
              "      <td>F</td>\n",
              "      <td>34</td>\n",
              "      <td>EU OBSERVATION</td>\n",
              "      <td>Private</td>\n",
              "      <td>English</td>\n",
              "      <td>WHITE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17765144</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2161</td>\n",
              "      <td>F</td>\n",
              "      <td>91</td>\n",
              "      <td>OBSERVATION ADMIT</td>\n",
              "      <td>Medicare</td>\n",
              "      <td>English</td>\n",
              "      <td>WHITE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows Ã— 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            label  log_stay_day  admit_year gender  age     admission_type  \\\n",
              "subject_id                                                                   \n",
              "16015722        1             2        2138      F   86           EW EMER.   \n",
              "11586654        1             2        2125      M   70       DIRECT EMER.   \n",
              "14039848        1             1        2153      F   56           EW EMER.   \n",
              "15103745        1             2        2185      F   81           EW EMER.   \n",
              "16077953        1             1        2182      M   91           EW EMER.   \n",
              "...           ...           ...         ...    ...  ...                ...   \n",
              "14551157        0             1        2161      F   52     EU OBSERVATION   \n",
              "13273553        0             3        2134      F   65           EW EMER.   \n",
              "15195372        0             1        2117      F   21  OBSERVATION ADMIT   \n",
              "17238479        0             1        2148      F   34     EU OBSERVATION   \n",
              "17765144        0             2        2161      F   91  OBSERVATION ADMIT   \n",
              "\n",
              "           insurance language                    race  \n",
              "subject_id                                             \n",
              "16015722    Medicare  English  BLACK/AFRICAN AMERICAN  \n",
              "11586654    Medicare  English                   WHITE  \n",
              "14039848    Medicaid  English                   WHITE  \n",
              "15103745    Medicare  English  BLACK/AFRICAN AMERICAN  \n",
              "16077953    Medicare  English                   WHITE  \n",
              "...              ...      ...                     ...  \n",
              "14551157     Private  English                   WHITE  \n",
              "13273553    Medicare  English                   WHITE  \n",
              "15195372     Private  English                   OTHER  \n",
              "17238479     Private  English                   WHITE  \n",
              "17765144    Medicare  English                   WHITE  \n",
              "\n",
              "[1000 rows x 9 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Predict if the patient will die in hospital:||True||False||\n",
            "Input: Patient EHR: log_stay_day is 2, admit_year is 2138, gender is F, age is 86, admission_type is EW EMER., insurance is Medicare, language is English, race is BLACK/AFRICAN AMERICAN\n",
            "Label (Output): 1\n"
          ]
        }
      ],
      "source": [
        "label_col = \"label\"\n",
        "prompt = \"Predict if the patient will die in hospital:||True||False||\"\n",
        "\n",
        "records = []\n",
        "for _, row in data.iterrows():\n",
        "    input_str = \"Patient EHR: \" + \", \".join(\n",
        "        [f\"{col} is {row[col]}\" for col in data.columns if col != label_col] # Perform text serialization\n",
        "    )\n",
        "    records.append({\n",
        "        \"instruction\": prompt,\n",
        "        \"input\": input_str,\n",
        "        \"output\": str(row[label_col])\n",
        "    })\n",
        "\n",
        "train_records, test_records = train_test_split(records, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create HuggingFace Datasets\n",
        "hf_dataset_train = Dataset.from_pandas(pd.DataFrame(train_records))\n",
        "hf_dataset_test = Dataset.from_pandas(pd.DataFrame(test_records))\n",
        "\n",
        "ex_record = records[0]\n",
        "# Full prompt + input structure for a single sample\n",
        "print(\"Prompt:\", ex_record['instruction'])\n",
        "print(\"Input:\", ex_record['input'])\n",
        "print(\"Label (Output):\", ex_record['output'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOc7R8GEDcba"
      },
      "source": [
        "#### Finetuning\n",
        "\n",
        "We will now load the pretrained model from huggingface and prepare it for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.2.\n",
            "   \\\\   /|    NVIDIA L40S. Num GPUs = 4. Max memory: 44.527 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "max_seq_length = 256\n",
        "\n",
        "# We will load in a pre-quantized model (4-bit precision), which is more memory efficient and faster to load and run.\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = torch.float16,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # [2.2] Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",], # [Question 2.2] This specifies what type of parameters in the LLM are being fine-tuned\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:00<00:00, 71422.80 examples/s]\n",
            "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:01<00:00, 794.95 examples/s]\n"
          ]
        }
      ],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token  # From your tokenizer\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    return {\n",
        "        \"text\": [\n",
        "            alpaca_prompt.format(inst, inp, out) + EOS_TOKEN\n",
        "            for inst, inp, out in zip(examples[\"instruction\"], examples[\"input\"], examples[\"output\"])\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# Apply formatting\n",
        "train_dataset = hf_dataset_train.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Set finetuning configurations\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 8, # Batch size\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 100, # Number of total training steps\n",
        "        learning_rate = 2e-4, # [Question 2.2] learning rate\n",
        "        fp16 = True,\n",
        "        bf16 = False,\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "dwfu8R0o4j27",
        "outputId": "06a4e0f0-5d55-4f00-d999-bcd17479168d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 800 | Num Epochs = 17 | Total steps = 100\n",
            "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 4 x 1) = 128\n",
            " \"-____-\"     Trainable parameters = 11,272,192/1,000,000,000 (1.13% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 01:23, Epoch 14/17]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.767800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.509900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.265800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.241000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.216400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.195100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.173000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.161700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.158300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.158500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Run to finetune model\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jv7nGV2f67sF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./lora_model/tokenizer_config.json',\n",
              " './lora_model/special_tokens_map.json',\n",
              " './lora_model/tokenizer.json')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save pretrained model to drive\n",
        "model.save_pretrained(\"./lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"./lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lniPW_UkFnr_"
      },
      "source": [
        "#### Inference\n",
        "\n",
        "We will now load the fine-tuned model and run inference on our test samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.2.\n",
            "   \\\\   /|    NVIDIA L40S. Num GPUs = 4. Max memory: 44.527 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 64811.93 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 6634.88 examples/s]\n"
          ]
        }
      ],
      "source": [
        "max_seq_length = 256\n",
        "\n",
        "# Only run this once if loading from saved model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"./lora_model\", # path to saved pretrained model\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = torch.float16,\n",
        "        load_in_4bit = True,\n",
        "    )\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "def formatting_prompts_func_test(examples):\n",
        "    # Format prompt strings first\n",
        "    texts = [\n",
        "        alpaca_prompt.format(inst, inp, \"\")\n",
        "        for inst, inp in zip(examples[\"instruction\"], examples[\"input\"])\n",
        "    ]\n",
        "\n",
        "    # Return just text for now, no tokenization or device ops here\n",
        "    return { \"text\": texts }\n",
        "\n",
        "# Apply formatting\n",
        "test_dataset = hf_dataset_test.map(formatting_prompts_func_test, batched=True)\n",
        "test_dataset = test_dataset.remove_columns([\"instruction\", \"input\", \"output\"])\n",
        "\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenized_test = test_dataset.map(\n",
        "    lambda x: tokenizer(\n",
        "        x[\"text\"], return_tensors=None, padding=True, truncation=True\n",
        "    ),\n",
        "    batched=True\n",
        ")\n",
        "tokenized_test = tokenized_test.remove_columns([\"text\"])\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
        "dataloader = DataLoader(tokenized_test, batch_size=8, collate_fn=data_collator, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "model.to(\"cuda\")\n",
        "\n",
        "generated_outputs = []\n",
        "log_likelihoods = []\n",
        "\n",
        "target_tokens = [\"False\", \"True\"]\n",
        "token_ids = tokenizer(target_tokens, add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "# These tokens correspond to \"True\" and \"False\"\n",
        "false_token_id = token_ids[0][0]\n",
        "true_token_id = token_ids[1][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO [Question 2.1]: run inference using fine-tuned model on test samples and collect predictions (logits)\n",
        "for i, batch in enumerate(dataloader):\n",
        "\n",
        "    input_ids = batch['input_ids'].to('cuda')\n",
        "    attention_mask = batch['attention_mask'].to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_logits=True,\n",
        "            return_dict_in_generate=True\n",
        "        )\n",
        "\n",
        "    logits = outputs.logits[0]  # shape: [batch_size, vocab_size]\n",
        "    # Compute log-softmax over vocab logits\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "    # Collect log-likelihoods of the True / False tokens\n",
        "    batch_false_log = log_probs[:, false_token_id]\n",
        "    batch_true_log = log_probs[:, true_token_id]\n",
        "    log_likelihoods.extend(torch.stack([batch_false_log, batch_true_log], dim=1).tolist())\n",
        "\n",
        "    # generated_ids = outputs.sequences  # shape: [batch_size, input_len + new_tokens]\n",
        "    # generated_outputs.extend() #Optional TODO: extract decoded tokens (if you want to compute accuracy / see text generation output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbRvJal8Ywbf",
        "outputId": "74a7de0f-612d-4655-e618-5963e6a9255d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUROC: 0.4175\n"
          ]
        }
      ],
      "source": [
        "y_true = [1 if label.strip().lower() == \"1\" else 0 for label in hf_dataset_test['output']]\n",
        "\n",
        "y_scores = [1 / (1 + math.exp(log_true))\n",
        "            for log_false, log_true in log_likelihoods]\n",
        "\n",
        "auroc = roc_auc_score(y_true, y_scores)\n",
        "print(f\"AUROC: {auroc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_OB4gA19AV1"
      },
      "source": [
        "## [30 Points] Question 2: Supervised Fine-tuning of LLMs\n",
        "\n",
        "#### [15 Points] 2.1: Complete the code for running inference and compute evaluation metrics of your choice\n",
        "\n",
        "\n",
        "#### [15 Points] 2.2: Repeat for 2 more configurations of fine-tuning parameters or prompting / serialization strategies, and justify your choices (the objective is to familiarize yourself with LoRA fine-tuning so this is not graded on model performance). Some options may be\n",
        "- Which parameters of the Llama model are fine-tuned\n",
        "- LoRA decomposition rank\n",
        "- Learning rate, regularization parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here I met with an issue that if I have already run the full pipeline once, the model can not be trained again in the same run. So I restart the kernel each time after I finished one full pipeline evaluation. If you need to rerun the code, please be advised to restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.2.\n",
            "   \\\\   /|    NVIDIA L40S. Num GPUs = 4. Max memory: 44.527 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "max_seq_length = 256\n",
        "\n",
        "# load in model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=torch.float16,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token  # From your tokenizer\n",
        "\n",
        "target_tokens = [\"False\", \"True\"]\n",
        "token_ids = tokenizer(target_tokens, add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "# These tokens correspond to \"True\" and \"False\"\n",
        "false_token_id = token_ids[0][0]\n",
        "true_token_id = token_ids[1][0]\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    return {\n",
        "        \"text\": [\n",
        "            alpaca_prompt.format(inst, inp, out) + EOS_TOKEN\n",
        "            for inst, inp, out in zip(examples[\"instruction\"], examples[\"input\"], examples[\"output\"])\n",
        "        ]\n",
        "    }\n",
        "\n",
        "def formatting_prompts_func_test(examples):\n",
        "    # Format prompt strings first\n",
        "    texts = [\n",
        "        alpaca_prompt.format(inst, inp, \"\")\n",
        "        for inst, inp in zip(examples[\"instruction\"], examples[\"input\"])\n",
        "    ]\n",
        "\n",
        "    # Return just text for now, no tokenization or device ops here\n",
        "    return { \"text\": texts }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 2.2[a] Change Rank\n",
        "\n",
        "Here we change the rank of the LoRA to a smaller value ($8$). Since the sample size is very limited in this fine-tuning setting, too many trainable parameters will make the model prone to overfit. Changing the rank can effectively reduce the trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.3.19 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5de60f08577b4d76813bbb0da1b62f7c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "738c118c216c495d930992b70529da1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/800 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 800 | Num Epochs = 17 | Total steps = 100\n",
            "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 4 x 1) = 128\n",
            " \"-____-\"     Trainable parameters = 5,636,096/1,000,000,000 (0.56% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 01:19, Epoch 14/17]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.754600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.492500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.256700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.231200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.200100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.174700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.161300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.158300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.156400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.157100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d77086c9140a467486bf6e8f3b014add",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "efe09ee1a6d94c5985f52385cdc8ece8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUROC: 0.4726\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=torch.float16,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 8, # [2.2] Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",], # [Question 2.2] This specifies what type of parameters in the LLM are being fine-tuned\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "# get train dataset\n",
        "train_dataset = hf_dataset_train.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Set finetuning configurations\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 8, # Batch size\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 100, # Number of total training steps\n",
        "        learning_rate = 2e-4, # [Question 2.2] learning rate\n",
        "        fp16 = True,\n",
        "        bf16 = False,\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")\n",
        "\n",
        "# start training\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "# go inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# get test dataset\n",
        "test_dataset = hf_dataset_test.map(formatting_prompts_func_test, batched=True)\n",
        "test_dataset = test_dataset.remove_columns([\"instruction\", \"input\", \"output\"])\n",
        "\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenized_test = test_dataset.map(\n",
        "    lambda x: tokenizer(\n",
        "        x[\"text\"], return_tensors=None, padding=True, truncation=True\n",
        "    ),\n",
        "    batched=True\n",
        ")\n",
        "tokenized_test = tokenized_test.remove_columns([\"text\"])\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
        "dataloader = DataLoader(tokenized_test, batch_size=8, collate_fn=data_collator, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "model.to(\"cuda\")\n",
        "\n",
        "# start inference\n",
        "log_likelihoods = []\n",
        "\n",
        "for i, batch in enumerate(dataloader):\n",
        "\n",
        "    input_ids = batch['input_ids'].to('cuda')\n",
        "    attention_mask = batch['attention_mask'].to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_logits=True,\n",
        "            return_dict_in_generate=True\n",
        "        )\n",
        "\n",
        "    logits = outputs.logits[0]  # shape: [batch_size, vocab_size]\n",
        "    # Compute log-softmax over vocab logits\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "    # Collect log-likelihoods of the True / False tokens\n",
        "    batch_false_log = log_probs[:, false_token_id]\n",
        "    batch_true_log = log_probs[:, true_token_id]\n",
        "    log_likelihoods.extend(torch.stack([batch_false_log, batch_true_log], dim=1).tolist())\n",
        "\n",
        "# output evaluation metrics\n",
        "y_true = [1 if label.strip().lower() == \"1\" else 0 for label in hf_dataset_test['output']]\n",
        "\n",
        "y_scores = [1 / (1 + math.exp(log_true))\n",
        "            for log_false, log_true in log_likelihoods]\n",
        "\n",
        "auroc = roc_auc_score(y_true, y_scores)\n",
        "print(f\"AUROC: {auroc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 2.2[b] Change Learning Rate\n",
        "\n",
        "Here we changed the learning rate to a smaller value ($1\\times 10^{-4}$) and the argument is similar to what have stated. I actually believe given such small training set, the model performance may not get better compared to zero-shot after the fine-tuning process. Here we change the model learning rate to let the model change little compared to the zero-shot model to avoid potential bias caused by the fine-tuning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.2.\n",
            "   \\\\   /|    NVIDIA L40S. Num GPUs = 4. Max memory: 44.527 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.3.19 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a612cc4becea4c68ac88c6440d1b9397",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78323ecaec6844e0b4a748255a47e29e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/800 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 800 | Num Epochs = 17 | Total steps = 100\n",
            "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 4 x 1) = 128\n",
            " \"-____-\"     Trainable parameters = 11,272,192/1,000,000,000 (1.13% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 01:21, Epoch 14/17]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.163700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.315000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.367900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.282700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.252400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.244400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.236300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.230900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.225800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.224400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79cdb5227aa6422dbd25a310782271e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed4046a2059f4b4cbb934f929257f6d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUROC: 0.4632\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=torch.float16,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # [2.2] Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",], # [Question 2.2] This specifies what type of parameters in the LLM are being fine-tuned\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "# get train dataset\n",
        "train_dataset = hf_dataset_train.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Set finetuning configurations\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 8, # Batch size\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 100, # Number of total training steps\n",
        "        learning_rate = 1e-4, # [Question 2.2] learning rate\n",
        "        fp16 = True,\n",
        "        bf16 = False,\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")\n",
        "\n",
        "# start training\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "# go inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# get test dataset\n",
        "test_dataset = hf_dataset_test.map(formatting_prompts_func_test, batched=True)\n",
        "test_dataset = test_dataset.remove_columns([\"instruction\", \"input\", \"output\"])\n",
        "\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenized_test = test_dataset.map(\n",
        "    lambda x: tokenizer(\n",
        "        x[\"text\"], return_tensors=None, padding=True, truncation=True\n",
        "    ),\n",
        "    batched=True\n",
        ")\n",
        "tokenized_test = tokenized_test.remove_columns([\"text\"])\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
        "dataloader = DataLoader(tokenized_test, batch_size=8, collate_fn=data_collator, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "model.to(\"cuda\")\n",
        "\n",
        "# start inference\n",
        "log_likelihoods = []\n",
        "\n",
        "for i, batch in enumerate(dataloader):\n",
        "\n",
        "    input_ids = batch['input_ids'].to('cuda')\n",
        "    attention_mask = batch['attention_mask'].to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_logits=True,\n",
        "            return_dict_in_generate=True\n",
        "        )\n",
        "\n",
        "    logits = outputs.logits[0]  # shape: [batch_size, vocab_size]\n",
        "    # Compute log-softmax over vocab logits\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "    # Collect log-likelihoods of the True / False tokens\n",
        "    batch_false_log = log_probs[:, false_token_id]\n",
        "    batch_true_log = log_probs[:, true_token_id]\n",
        "    log_likelihoods.extend(torch.stack([batch_false_log, batch_true_log], dim=1).tolist())\n",
        "\n",
        "# output evaluation metrics\n",
        "y_true = [1 if label.strip().lower() == \"1\" else 0 for label in hf_dataset_test['output']]\n",
        "\n",
        "y_scores = [1 / (1 + math.exp(log_true))\n",
        "            for log_false, log_true in log_likelihoods]\n",
        "\n",
        "auroc = roc_auc_score(y_true, y_scores)\n",
        "print(f\"AUROC: {auroc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U90OHck6S6L"
      },
      "source": [
        "## [40 Points] Question 3: Autoregressive Sequence Modeling\n",
        "\n",
        "We will now explore the conceptual foundations of pretraining sequence models, and the various emergent behaviours that arise from modeling sequences at scale.\n",
        "\n",
        "Consider a dataset of $N$ sequences, each sequence with a maximum length $T$:\n",
        "\n",
        "$$D = \\{(x_1^i, x_2^i,..., x_{T}^i) \\}_{i=1}^N$$\n",
        "\n",
        "For purposes of this problem, we will assume tokens $x_t \\in R^1$ are univariate and continuous-valued. We will consider a sequence model $p_\\theta$ that specifies the one-step conditional densities $p_\\theta(x_{t+1} | x_{1:t})$. (note here that $x_{1:t}$ means all $x$ from timestep $1$ to $t$, which we can also write as $x_{\\leq t}$)\n",
        "\n",
        "We will now derive the optimization problem for learning such a model given the dataset $D$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### [15 Points] 3.1: Show that the maximizing the likelihood of the empirical data distribution is equivalent to minimizing the following empirical loss function\n",
        "$$\n",
        "\\begin{align}\n",
        "l(p_\\theta, D) = - \\sum_{i=1}^N \\sum_{t=1}^T \\log p_\\theta(x_{t+1}^i | x_{1:t}^i)\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font color=\"red\">Answer 3.1</font>\n",
        "\n",
        "Suppose all the sequences are sampled iid.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\text{liklihood}(\\theta | D) &= \\prod_{i=1}^{N} p_{\\theta}(x^{i}_1, x^{i}_2, ..., x^{i}_T) \\\\\n",
        "&= \\prod_{i=1}^{N} \\prod_{t=0}^{T-1} p_{\\theta}(x^{i}_{t + 1} | x^{i}_1, ..., x^{i}_t) \\\\\n",
        "\\text{log-liklihood}(\\theta | D) &= \\sum_{i=1}^{N} \\sum_{t=0}^{T-1} \\log p_{\\theta}(x^{i}_{t + 1} | x^{i}_{1:t})\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "So the maximizing log-liklihood esentially is the same with minimizing the loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### [15 Points] 3.2 Now assume that each one-step conditional density is parameterized as a Gaussian distribution i.e.\n",
        "\n",
        "$$ p_\\theta(x_{t+1} | x_{1:t}) = N(\\mu_t, \\sigma^2_t) $$\n",
        "\n",
        "#### where each $\\mu_t = f_\\theta(x_{1:t})$ and $\\sigma^2_t = g_\\theta(x_{1:t})$ are mappings from the sequence model (ex. a transformer model). Write the loss function from 1.1 in terms of these parameters. What is the interpretation of the $\\mu_t$ and $\\sigma^2_t$ parameters? What would be the benefit in increasing the maximum context length $T$ during pretraining and inference?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font color=\"red\">Answer 3.2</font>\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "l(p_\\theta, D) &= - \\sum_{i=1}^N \\sum_{t=0}^{T-1} \\log p_\\theta(x_{t+1}^i | x_{1:t}^i) \\\\\n",
        "&= - \\sum_{i=1}^N \\sum_{t=0}^{T-1} \\log \\mathcal{N}(x_{t+1}^i | \\mu_t, \\sigma^2_t) \\\\\n",
        "&= \\sum_{i=1}^N \\sum_{t=0}^{T-1} \\left[ \\frac{(x_{t+1}^i - \\mu_t)^2}{2\\sigma_t^2} + \\frac{1}{2} \\log \\sigma_t^2 \\right] + \\textit{const.}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Here, $\\mu_t$ is the predicted mean of the next token $x_{t+1}$ given all the previous token. It can be interpreted as the model's best guess for the next token given the context. $\\sigma^2_t$ is the predicted variance of the next token $x_{t+1}$ given all the previous token. It can be interpreted as the model's uncertainty of the next token $x_{t+1}$ given all the previous token.\n",
        "\n",
        "If we let the $T$ increase, during the training stage, we can learn longer-range dependencies in the data, which can help model capture complex patterns in the context. During the inference stage, this allows the model to leverage richer contextual information for predictions, enhancing model performance. In this senerio, We also have a by product that larger the $T$ is, more parameters of the model we will have, which potentially gives the model more flexibility but also puts the model into risk of overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### [10 Points] 3.3 Using a specific healthcare application of your choice, discuss one potential issue in pretraining such a model on a large corpus of publicly available data (ex. robustness to distribution shifts, safety & alignment, data biases & fairness). The following paper can serve as a starting point for some ideas, but feel free to explore the literature in this field:\n",
        "\n",
        "On the Opportunities and Risks of\n",
        "Foundation Models: https://arxiv.org/pdf/2108.07258\n",
        "\n",
        "(Note: One paragraph is sufficient, cite any external sources if you use any)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font color=\"red\">Answer 3.3</font>\n",
        "\n",
        "Pretraining a model on publicly available clinical notes (e.g., from academic hospitals or specific regions) can be problematic as publicly available domain-specific data is really limited and this limitation will also cause potential model bias. High-quality, diverse clinical notes are rarely fully public due to privacy laws (e.g., HIPAA). Models trained on limited, biased corpora (e.g., MIMIC, which skews toward ICU patients) inherit these limitations. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "unsloth",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04ef450650dc4fcba9b27fd673ff74e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_974c8c32e0ca4224bd3be3cc1b2b98c0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4464438d63c4440a90abce61b5ed8d57",
            "value": "â€‡174/174â€‡[00:00&lt;00:00,â€‡5037.19â€‡examples/s]"
          }
        },
        "08b84b3690fc4cb9b1978e01010ab61a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ff79a9e9b5540c486570dbffd194851",
              "IPY_MODEL_b64e80449c83403a9d4c41a6fbd02a4c",
              "IPY_MODEL_c5396effe08a414898ccd856a0b7656c"
            ],
            "layout": "IPY_MODEL_199499e4e4cb4e44956ccac4fee58c43"
          }
        },
        "0a1d14bf737a41888e6831db91430339": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de7bd493c2904ddeb93cf797e73e2f8d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6e4abdcc01a4467e9bcf1239b0a2d8bb",
            "value": "Map:â€‡100%"
          }
        },
        "199499e4e4cb4e44956ccac4fee58c43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1de428c2b32f465b8edcb2a83cc9d161": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e7d3952bfbc48c589a25f18abeacfc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a8389a633594d8b96affa13d95daa58",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_eaa9e04623d9491aa3ae50be7e150234",
            "value": "Connecting..."
          }
        },
        "2467cfbd9934407fa8937fe33583a6d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24d78c031b1c495785f7dc26e955abff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_cdaa21193290490bab9f54dffb0e83c1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1de428c2b32f465b8edcb2a83cc9d161",
            "value": ""
          }
        },
        "2b5949bdb7474ffb81fa04b5b1af815e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fdde398480b43208d21e155b0bcee89": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41c9605f75d241b787832da341286708": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4464438d63c4440a90abce61b5ed8d57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ff79a9e9b5540c486570dbffd194851": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da0d8c6aea3b4722b69d6612b5cd812a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7fb2bbe073c04548acf9671c0566b022",
            "value": "Map:â€‡100%"
          }
        },
        "579e08dbbb61430d9ef87312ffe69495": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_79cc4c531d7f4533a2d72db3acbaeb3b"
          }
        },
        "5a8389a633594d8b96affa13d95daa58": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cb021c9154b4285b676b31e1a6ca000": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61f1455774844b76bfee5edcb6f24d76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cece86628fe4628bc3d62eb455485a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_e00914db59524b109a1641a0361fd76c",
            "style": "IPY_MODEL_7fac383a49a54d73b5a74de5adc2b415",
            "value": true
          }
        },
        "6e4abdcc01a4467e9bcf1239b0a2d8bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71e89720a7b74955bd7348f911f6e1e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "73964a8b31e741b29fa51ca7e176a82a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79cc4c531d7f4533a2d72db3acbaeb3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "7fac383a49a54d73b5a74de5adc2b415": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fb2bbe073c04548acf9671c0566b022": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "837a18955e2d4c3a8cd0783c94c2c071": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "974c8c32e0ca4224bd3be3cc1b2b98c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a24571830679429ca29c4b9e7e72fade": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_73964a8b31e741b29fa51ca7e176a82a",
            "style": "IPY_MODEL_a80f7395a1b8485c963186e8e0833bc6",
            "tooltip": ""
          }
        },
        "a80f7395a1b8485c963186e8e0833bc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "ae2f3757273d4922b20c579941f00db4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b28b750b5665480bbbd0fbf817c82899": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fdde398480b43208d21e155b0bcee89",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b929d93407ec46b1bcd6d35c02a96075",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "b64102ca613b4a6c8c8723d54a3e0073": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cb021c9154b4285b676b31e1a6ca000",
            "max": 174,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e94ef29a260240bfb9d942e0cd530bf7",
            "value": 174
          }
        },
        "b64e80449c83403a9d4c41a6fbd02a4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae2f3757273d4922b20c579941f00db4",
            "max": 174,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71e89720a7b74955bd7348f911f6e1e9",
            "value": 174
          }
        },
        "b929d93407ec46b1bcd6d35c02a96075": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5396effe08a414898ccd856a0b7656c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61f1455774844b76bfee5edcb6f24d76",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2b5949bdb7474ffb81fa04b5b1af815e",
            "value": "â€‡174/174â€‡[00:00&lt;00:00,â€‡2345.17â€‡examples/s]"
          }
        },
        "cdaa21193290490bab9f54dffb0e83c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cecf6e38df394a3780a9b868a05742e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a1d14bf737a41888e6831db91430339",
              "IPY_MODEL_b64102ca613b4a6c8c8723d54a3e0073",
              "IPY_MODEL_04ef450650dc4fcba9b27fd673ff74e9"
            ],
            "layout": "IPY_MODEL_41c9605f75d241b787832da341286708"
          }
        },
        "da0d8c6aea3b4722b69d6612b5cd812a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbc7d52db2b54658b531c2fe2bb6f161": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2467cfbd9934407fa8937fe33583a6d2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_837a18955e2d4c3a8cd0783c94c2c071",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "de7bd493c2904ddeb93cf797e73e2f8d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e00914db59524b109a1641a0361fd76c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e94ef29a260240bfb9d942e0cd530bf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eaa9e04623d9491aa3ae50be7e150234": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
